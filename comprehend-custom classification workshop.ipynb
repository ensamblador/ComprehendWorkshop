{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of Amazon Comprehend to provide text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK. **Note: This role should have AmazonComprehendFullAccess, so it can create and run custom classification jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::625941629713:role/service-role/AmazonSageMaker-ExecutionRole-20190415T173068\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch, Comprehend) on your behalf\n",
    "bucket='comprehend-demolm' # customize to your bucket\n",
    "prefix = 'dbpedia/' #Replace with the prefix under which you want to store the data if needed\n",
    "region = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Now we'll download a dataset from the web on which we want to train the text classification model. BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\__label\\__\".\n",
    "\n",
    "In this example, let us train the text classification model on the [DBPedia Ontology Dataset](https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014#2) as done by [Zhang et al](https://arxiv.org/pdf/1509.01626.pdf). The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. It has 560,000 training samples and 70,000 testing samples. The fields we used for this dataset contain title and abstract of each Wikipedia article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-20 18:28:53--  https://github.com/saurabh3949/Text-Classification-Datasets/raw/master/dbpedia_csv.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/dbpedia_csv.tar.gz [following]\n",
      "--2019-07-20 18:28:53--  https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/dbpedia_csv.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68431223 (65M) [application/octet-stream]\n",
      "Saving to: ‘dbpedia_csv.tar.gz’\n",
      "\n",
      "dbpedia_csv.tar.gz  100%[===================>]  65.26M  87.8MB/s    in 0.7s    \n",
      "\n",
      "2019-07-20 18:28:55 (87.8 MB/s) - ‘dbpedia_csv.tar.gz’ saved [68431223/68431223]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/saurabh3949/Text-Classification-Datasets/raw/master/dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbpedia_csv/\n",
      "dbpedia_csv/test.csv\n",
      "dbpedia_csv/classes.txt\n",
      "dbpedia_csv/train.csv\n",
      "dbpedia_csv/readme.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the dataset and the classes to get some understanding about how the data and the label is provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\"E. D. Abbott Ltd\",\" Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\"\r\n",
      "1,\"Schwan-Stabilo\",\" Schwan-STABILO is a German maker of pens for writing colouring and cosmetics as well as markers and highlighters for office use. It is the world's largest manufacturer of highlighter pens Stabilo Boss.\"\r\n",
      "1,\"Q-workshop\",\" Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice and dice accessories for use in various games (role-playing gamesboard games and tabletop wargames). They also run an online retail store and maintainan active forum community.Q-workshop was established in 2001 by Patryk Strzelewicz – a student from Poznań. Initiallythe company sold its products via online auction services but in 2005 a website and online store wereestablished.\"\r\n"
     ]
    }
   ],
   "source": [
    "!head dbpedia_csv/train.csv -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above output, the CSV has 3 fields - Label index, title and abstract. Let us first create a label index to label name mapping and then proceed to preprocess the dataset for ingestion by BlazingText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will print the labels file (`classes.txt`) to see all possible labels followed by creating an index to label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\r\n",
      "EducationalInstitution\r\n",
      "Artist\r\n",
      "Athlete\r\n",
      "OfficeHolder\r\n",
      "MeanOfTransportation\r\n",
      "Building\r\n",
      "NaturalPlace\r\n",
      "Village\r\n",
      "Animal\r\n",
      "Plant\r\n",
      "Album\r\n",
      "Film\r\n",
      "WrittenWork\r\n"
     ]
    }
   ],
   "source": [
    "!cat dbpedia_csv/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the mapping from integer indices to class label which will later be used to retrieve the actual class name during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Company', '2': 'EducationalInstitution', '3': 'Artist', '4': 'Athlete', '5': 'OfficeHolder', '6': 'MeanOfTransportation', '7': 'Building', '8': 'NaturalPlace', '9': 'Village', '10': 'Animal', '11': 'Plant', '12': 'Album', '13': 'Film', '14': 'WrittenWork'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {} \n",
    "with open(\"dbpedia_csv/classes.txt\") as f:\n",
    "    for i,label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i+1)] = label.strip()\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We need to preprocess the training data into **space separated tokenized text** format which can be consumed by Amazon Comprehend. Also, as mentioned previously, the class label(s) will be mapped from the classes.txt into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = ''\n",
    "    cur_row = index_to_label[row] \n",
    "    return cur_row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, testfile=1):\n",
    "    all_rows = ''\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        #csv_reader = csv.reader(csvinfile, delimiter='\\n')\n",
    "        count = 0\n",
    "        for row in csvinfile:\n",
    "            if (testfile == 0):\n",
    "                count += 1;\n",
    "                if (count == 200):\n",
    "                    break\n",
    "            category = row.split(',')[0]\n",
    "            title = row.split(',')[1]\n",
    "            document = row.split(title+',')[1]\n",
    "            all_rows += transform_instance(category) + ',' + document\n",
    "    \n",
    "        with open(output_file, 'w') as csvoutfile:\n",
    "            csvoutfile.write(all_rows)\n",
    "            \n",
    "def preprocesstest(input_file, output_file, testfile=1):\n",
    "    all_rows = ''\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        #csv_reader = csv.reader(csvinfile, delimiter='\\n')\n",
    "        count = 0\n",
    "        for row in csvinfile:\n",
    "            if (testfile == 0):\n",
    "                count += 1;\n",
    "                if (count == 200):\n",
    "                    break\n",
    "            title = row.split(',')[1]\n",
    "            document = row.split(title+',')[1]\n",
    "            all_rows += document\n",
    "    \n",
    "        with open(output_file, 'w') as csvoutfile:\n",
    "            csvoutfile.write(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 s, sys: 632 ms, total: 3.21 s\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "preprocess('dbpedia_csv/train.csv', 'dbpedia.train')\n",
    "        \n",
    "# Preparing the test dataset        \n",
    "preprocesstest('dbpedia_csv/test.csv', 'dbpedia.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company,\" Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\"\r\n",
      "Company,\" Schwan-STABILO is a German maker of pens for writing colouring and cosmetics as well as markers and highlighters for office use. It is the world's largest manufacturer of highlighter pens Stabilo Boss.\"\r\n",
      "Company,\" Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice and dice accessories for use in various games (role-playing gamesboard games and tabletop wargames). They also run an online retail store and maintainan active forum community.Q-workshop was established in 2001 by Patryk Strzelewicz – a student from Poznań. Initiallythe company sold its products via online auction services but in 2005 a website and online store wereestablished.\"\r\n"
     ]
    }
   ],
   "source": [
    "!head dbpedia.train -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# caltech-256\n",
    "s3_train_key = \"dbpedia/train\"\n",
    "s3_test_key = \"dbpedia/test\"\n",
    "\n",
    "upload_to_s3(s3_train_key, 'dbpedia.train')\n",
    "upload_to_s3(s3_test_key, 'dbpedia.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}output'.format(bucket, prefix)\n",
    "s3_train_location = 's3://{}/{}train'.format(bucket, prefix)+'/'+'dbpedia.train'\n",
    "s3_test_location = 's3://{}/{}test'.format(bucket, prefix)+'/'+'dbpedia.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://comprehend-demolm/dbpedia/output\n",
      "s3://comprehend-demolm/dbpedia/train/dbpedia.train\n",
      "dbpedia/train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (s3_output_location)\n",
    "print (s3_train_location)\n",
    "print (s3_train_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Comprehend for custom classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Policy for Comprehend Service role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::625941629713:policy/Comprehendpolicy\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "policy_name = \"Comprehendpolicy\"\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"VisualEditor0\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:DeleteObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::*Comprehend*\",\n",
    "                \"arn:aws:s3:::*comprehend*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName = policy_name,\n",
    "    PolicyDocument = json.dumps(policy_document),\n",
    "    Description='Comprehend Policy'\n",
    ")\n",
    "PolicyArn=create_policy_response[\"Policy\"][\"Arn\"]\n",
    "print(PolicyArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::625941629713:role/ComprehendRole\n"
     ]
    }
   ],
   "source": [
    "role_name = \"ComprehendRole\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"comprehend.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document),\n",
    "    Description='Amazon Comprehend service role for classifier.'\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = create_policy_response[\"Policy\"][\"Arn\"]\n",
    ")\n",
    "\n",
    "time.sleep(30) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a custom classification training job using the Boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create response: %s\n",
      " {'DocumentClassifierArn': 'arn:aws:comprehend:us-east-1:625941629713:document-classifier/dbpedia-classifier', 'ResponseMetadata': {'RequestId': '3dacfe45-41a1-4c43-8343-706050089613', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3dacfe45-41a1-4c43-8343-706050089613', 'content-type': 'application/x-amz-json-1.1', 'content-length': '108', 'date': 'Sat, 20 Jul 2019 18:32:27 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Boto3 SDK:\n",
    "client = boto3.client('comprehend', region_name=(region))\n",
    "\n",
    "# Create a document classifier\n",
    "create_response = client.create_document_classifier(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': (s3_train_location)\n",
    "    },\n",
    "    DataAccessRoleArn=(role_arn),\n",
    "    DocumentClassifierName='dbpedia-classifier',\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print(\"Create response: %s\\n\", create_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentClassifierProperties: SUBMITTED   (elapsed = 0:00:09.765044)\n",
      "DocumentClassifierProperties: SUBMITTED   (elapsed = 0:00:24.881853)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:00:39.955981)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:00:55.020844)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:01:10.088692)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:01:25.163460)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:01:40.247051)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:01:55.284898)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:02:10.351946)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:02:25.430562)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:02:40.511459)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:02:55.589646)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:03:10.677079)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:03:25.719026)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:03:40.799163)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:03:55.864135)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:04:10.944883)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:04:25.989421)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:04:41.073240)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:04:56.162617)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:05:11.215758)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:05:26.274660)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:05:41.356450)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:05:56.391256)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:06:11.430098)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:06:26.478489)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:06:41.542948)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:06:56.619589)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:07:11.668424)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:07:26.755093)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:07:41.800679)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:07:56.881563)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:08:11.959653)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:08:27.009553)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:08:42.051674)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:08:57.130919)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:09:12.170133)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:09:27.242485)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:09:42.309491)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:09:57.389115)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:10:12.469512)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:10:27.545011)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:10:42.605907)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:10:57.670830)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:11:12.759831)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:11:27.801177)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:11:42.885458)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:11:57.931016)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:12:13.035657)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:12:28.095433)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:12:43.142593)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:12:58.186171)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:13:13.299819)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:13:28.368355)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:13:43.543532)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:13:58.623946)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:14:13.680543)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:14:28.745683)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:14:43.804945)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:14:58.880772)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:15:13.961115)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:15:29.066221)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:15:44.113581)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:15:59.175747)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:16:14.264284)\n",
      "DocumentClassifierProperties: TRAINING   (elapsed = 0:16:29.329745)\n",
      "DocumentClassifierProperties: TRAINED   (elapsed = 0:16:44.412295)\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_response = client.describe_document_classifier(\n",
    "    DocumentClassifierArn=create_response['DocumentClassifierArn'])\n",
    "    status = describe_response[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_response[\"DocumentClassifierProperties\"][\"SubmitTime\"]\n",
    "    print(\"DocumentClassifierProperties: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inference\n",
    "Once the training is done, we can create a job to classify documents with the Amazon Comprehend custom classifier. We will run this against our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start response: %s\n",
      " {'JobId': '1d61584cb891b55c49c1864d2d356081', 'JobStatus': 'SUBMITTED', 'ResponseMetadata': {'RequestId': '93c08869-abc6-49c7-af20-eec1173c582f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '93c08869-abc6-49c7-af20-eec1173c582f', 'content-type': 'application/x-amz-json-1.1', 'content-length': '68', 'date': 'Sat, 20 Jul 2019 18:50:02 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "start_response = client.start_document_classification_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': (s3_test_location),\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': (s3_output_location)\n",
    "    },\n",
    "    DataAccessRoleArn='arn:aws:iam::625941629713:role/service-role/AmazonComprehendServiceRole-dbpedia',\n",
    "    DocumentClassifierArn='arn:aws:comprehend:us-east-1:625941629713:document-classifier/dbpedia-classifier'\n",
    ")\n",
    "\n",
    "print(\"Start response: %s\\n\", start_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:00:08.044773)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:00:23.101194)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:00:38.145841)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:00:53.208978)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:01:08.278999)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:01:23.346907)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:01:38.421726)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:01:53.501449)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:02:08.559914)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:02:23.602743)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:02:38.673054)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:02:53.728880)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:03:08.788769)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:03:23.818664)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:03:38.886298)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:03:53.964074)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:04:09.014608)\n",
      "DocumentClassificationJobProperties: IN_PROGRESS   (elapsed = 0:04:24.057065)\n",
      "DocumentClassificationJobProperties: COMPLETED   (elapsed = 0:04:39.123303)\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_response = client.describe_document_classification_job(JobId=start_response['JobId'])\n",
    "    status = describe_response[\"DocumentClassificationJobProperties\"][\"JobStatus\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - describe_response[\"DocumentClassificationJobProperties\"][\"SubmitTime\"]\n",
    "    print(\"DocumentClassificationJobProperties: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)\n",
    "output_location = describe_response[\"DocumentClassificationJobProperties\"][\"OutputDataConfig\"]\n",
    "outputs3=output_location[\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocumentClassificationJobProperties': {'JobId': '1d61584cb891b55c49c1864d2d356081', 'JobStatus': 'COMPLETED', 'SubmitTime': datetime.datetime(2019, 7, 20, 18, 50, 3, 398000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2019, 7, 20, 18, 54, 39, 268000, tzinfo=tzlocal()), 'DocumentClassifierArn': 'arn:aws:comprehend:us-east-1:625941629713:document-classifier/dbpedia-classifier', 'InputDataConfig': {'S3Uri': 's3://comprehend-demolm/dbpedia/test/dbpedia.test', 'InputFormat': 'ONE_DOC_PER_LINE'}, 'OutputDataConfig': {'S3Uri': 's3://comprehend-demolm/dbpedia/output/625941629713-CLN-1d61584cb891b55c49c1864d2d356081/output/output.tar.gz'}, 'DataAccessRoleArn': 'arn:aws:iam::625941629713:role/service-role/AmazonComprehendServiceRole-dbpedia'}, 'ResponseMetadata': {'RequestId': '67e90f01-ab2b-4824-a3d3-2956d9a26d38', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '67e90f01-ab2b-4824-a3d3-2956d9a26d38', 'content-type': 'application/x-amz-json-1.1', 'content-length': '626', 'date': 'Sat, 20 Jul 2019 18:54:42 GMT'}, 'RetryAttempts': 0}}\n",
      "s3://comprehend-demolm/dbpedia/output/625941629713-CLN-1d61584cb891b55c49c1864d2d356081/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "output_location = describe_response[\"DocumentClassificationJobProperties\"][\"OutputDataConfig\"]\n",
    "print(describe_response)\n",
    "outputs3=output_location[\"S3Uri\"]\n",
    "print(outputs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classification job has run lets download and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://comprehend-demolm/dbpedia/output/625941629713-CLN-1d61584cb891b55c49c1864d2d356081/output/output.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "s3 = (outputs3)\n",
    "!echo $dir_path\n",
    "\n",
    "#!aws s3 cp $1 . --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
